#!{{ virtualenv_dir }}/bin/python
import sys
import yaml
import argparse
import datetime

from influxdb import InfluxDBClient
import psycopg2
import xml.etree.ElementTree as ET
import re
import subprocess

add_queue_info = {{ add_queue_info is truthy }}
add_utilisation_info = {{ add_utilisation_info is truthy }}
stats_dry_run_only = {{ stats_dry_run_only is truthy }}

sinfo_hostname = '{{ sinfo_hostname }}'
secrets_file = '{{ stats_dir }}/secret.yml'
job_conf_path = '{{ galaxy_config_dir }}/job_conf.yml'

secrets = yaml.safe_load(open(secrets_file, 'r'))
SALT = secrets['salt']
TOP_USAGE_LIMIT = 30
PGCONNS = secrets['pgconn']

time = datetime.datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')


def eprint(*args, **kwargs):
    print(*args, file=sys.stderr, **kwargs)


def get_destinations(job_conf):
    with open(job_conf) as handle:
        destinations = list(yaml.safe_load(handle).get('execution', {}).get('environments', []).keys())
    return destinations 


def queue_monitor_v2():
    return """
        SELECT
            tool_id, tool_version, destination_id, handler, state, job_runner_name, count(*) as count
        FROM job
        WHERE
            state = 'running' or state = 'queued'
        GROUP BY
            tool_id, tool_version, destination_id, handler, state, job_runner_name;
    """


def make_measurement(measurement, value, tags=None):
    m = {
        'measurement': measurement,
        'time': time,
        'fields': {
            'value': value
        }
    }
    if tags:
        m['tags'] = tags
    return m


def pg_execute(pconn_str, sql):
    pconn = psycopg2.connect(pconn_str)
    pc = pconn.cursor()
    pc.execute(sql)
    for row in pc:
        yield row
    pconn.close()


def collect(instance, dests):
    measurements = []
    queued = 0
    running = 0
    dest_counts = {}
    for dest in dests:
        dest_counts[dest] = {'queued': 0, 'running': 0}

    pconn_str = PGCONNS[instance]
    for row in pg_execute(pconn_str, queue_monitor_v2()):
        tags = {
            'tool': row[0],
            'tool_version': row[1],
            'destination': row[2],
            'handler': row[3],
            'state': row[4],
            'cluster': row[5],
        }
        if row[4] == 'queued' or row[4] == 'running':
            if row[2] is not None:  # occasionally jobs will enter 'queued' state without destinations
                dest_counts[row[2]][row[4]] += row[6]
        if row[4] == 'queued':
            queued += row[6]
        if row[4] == 'running':
            running += row[6]
        measurements.append(make_measurement('job_queue', float(row[6]), tags=tags))
    for dc in dest_counts:
        tags = {'destination': dc, 'state': 'running'}
        measurements.append(make_measurement('queue_totals', float(dest_counts[dc]['running']), tags=tags))
        tags = {'destination': dc, 'state': 'queued'}
        measurements.append(make_measurement('queue_totals', float(dest_counts[dc]['queued']), tags=tags))
    tags = {'state': 'queued'}
    measurements.append(make_measurement('queue_agg', float(queued), tags=tags))
    tags = {'state': 'running'}
    measurements.append(make_measurement('queue_agg', float(running), tags=tags))
    return measurements


def get_sinfo():
    total_cpus = 0
    cpus_allocated = 0
    m = []
    sout = subprocess.check_output(['sinfo']).decode('utf-8').split('\n')
    #eprint(sout)
    for line in sout[1:]: # skip header line
        if not line: # skip empty lines
            continue
        node, alloc_info, node_state = re.split('\s+', line)
        node_alloc = float(alloc_info.split('/')[0])
        node_cpus = float(alloc_info.split('/')[3])
        node_percent = node_alloc / node_cpus * 100.0
        total_cpus += node_cpus
        cpus_allocated += node_alloc
        m.append({ 
            'measurement': 'node_sinfo', 
            'time': time, 
            'fields': { 
                'node_alloc': node_alloc, 
                'node_cpus': node_cpus, 
                'node_percent': node_percent,
                'node_state': node_state
                },
            'tags': {
                'host': sinfo_hostname,
                'node': node
                }
            })
    percent = cpus_allocated / total_cpus * 100.0
    m.append({
        'measurement': 'sinfo',
        'time': time,
        'fields': {
            'total_cpus': total_cpus,
            'cpus_allocated': cpus_allocated,
            'percent_allocated': percent
        },
        'tags': {
            'host': sinfo_hostname
        }
    })
    return m


def dump(instance, points):
    client = InfluxDBClient(**secrets['influxdb'])
    client.write_points(points)


def main():
    parser = argparse.ArgumentParser(description="Translate Galaxy DB stats from PostgreSQL to InfluxDB")
    parser.add_argument('-d', '--dry_run', action='store_true', help="Print to screen, do not post to influx")
    parser.add_argument('-i', '--instance', default='main', help="Galaxy instance")
    parser.add_argument('-j', '--job_conf', default=job_conf_path, help="The Galaxy Job Conf file")
    args = parser.parse_args()
    if add_queue_info:
        destinations = get_destinations(args.job_conf)
        points = collect(args.instance, destinations)
        if not (args.dry_run or stats_dry_run_only):
            dump(args.instance, points)
        else:
            print(points)
    if add_utilisation_info:
        sinfo = get_sinfo()
        if not (args.dry_run or stats_dry_run_only):
            dump(args.instance, sinfo)
        else:
            print(sinfo)


if __name__ == '__main__':
    main()
